{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import KeyedVectors\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(stopwords_path):\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]\n",
    "    \n",
    "def preprocess_data(corpus_path, stopwords):\n",
    "    corpus = []\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = re.sub(r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:。？、~@#￥%……&*（）]+\", \" \", line)\n",
    "            corpus.append(' '.join([word for word in jieba.lcut(line) if word != \" \" and word != \"\\t\" and word not in stopwords]))\n",
    "    return corpus\n",
    "\n",
    "def cut_low_freq_word(documents, thre):\n",
    "    texts = [[word for word in document.split()] for document in documents]\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    return [[token for token in text if frequency[token] > thre] for text in texts]\n",
    "\n",
    "def cut_low_freq_word2(documents, thre):\n",
    "    texts = [[word for word in document.split()] for document in documents]\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    return [(\" \").join([token for token in text if frequency[token] > thre]) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = \"./data/stop_words.txt\"\n",
    "doc_title_east = \"./data/title_east.txt\"\n",
    "doc_title_apple= \"./data/title_apple.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-07-27 15:52:00,163 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/m3/4yh806w92fdgcn0bk16ql7nw0000gn/T/jieba.cache\n",
      "2020-07-27 15:52:00,166 : DEBUG : Loading model from cache /var/folders/m3/4yh806w92fdgcn0bk16ql7nw0000gn/T/jieba.cache\n",
      "Loading model cost 0.632 seconds.\n",
      "2020-07-27 15:52:00,796 : DEBUG : Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2020-07-27 15:52:00,797 : DEBUG : Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "stopwords = load_stopwords(stopwords_path)\n",
    "corp_title_east = preprocess_data(doc_title_east, stopwords)\n",
    "corp_title_apple = preprocess_data(doc_title_apple, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_title_east = cut_low_freq_word(corp_title_east, 3)\n",
    "texts_title_apple = cut_low_freq_word(corp_title_apple, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 15:52:21,747 : INFO : collecting all words and their counts\n",
      "2020-07-27 15:52:21,748 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-07-27 15:52:21,761 : INFO : PROGRESS: at sentence #10000, processed 67573 words, keeping 7657 word types\n",
      "2020-07-27 15:52:21,776 : INFO : PROGRESS: at sentence #20000, processed 137486 words, keeping 9275 word types\n",
      "2020-07-27 15:52:21,796 : INFO : PROGRESS: at sentence #30000, processed 200510 words, keeping 9961 word types\n",
      "2020-07-27 15:52:21,813 : INFO : PROGRESS: at sentence #40000, processed 262261 words, keeping 10177 word types\n",
      "2020-07-27 15:52:21,827 : INFO : PROGRESS: at sentence #50000, processed 323506 words, keeping 10250 word types\n",
      "2020-07-27 15:52:21,837 : INFO : collected 10262 word types from a corpus of 365311 raw words and 57222 sentences\n",
      "2020-07-27 15:52:21,838 : INFO : Loading a fresh vocabulary\n",
      "2020-07-27 15:52:21,852 : INFO : effective_min_count=5 retains 8804 unique words (85% of original 10262, drops 1458)\n",
      "2020-07-27 15:52:21,852 : INFO : effective_min_count=5 leaves 359479 word corpus (98% of original 365311, drops 5832)\n",
      "2020-07-27 15:52:21,877 : INFO : deleting the raw counts dictionary of 10262 items\n",
      "2020-07-27 15:52:21,878 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2020-07-27 15:52:21,879 : INFO : downsampling leaves estimated 305462 word corpus (85.0% of prior 359479)\n",
      "2020-07-27 15:52:21,901 : INFO : estimated required memory for 8804 words and 300 dimensions: 25531600 bytes\n",
      "2020-07-27 15:52:21,902 : INFO : resetting layer weights\n",
      "2020-07-27 15:52:23,548 : INFO : training model with 3 workers on 8804 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-07-27 15:52:23,843 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:52:23,848 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:52:23,851 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:52:23,852 : INFO : EPOCH - 1 : training on 365311 raw words (305245 effective words) took 0.3s, 1052857 effective words/s\n",
      "2020-07-27 15:52:24,121 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:52:24,127 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:52:24,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:52:24,129 : INFO : EPOCH - 2 : training on 365311 raw words (305440 effective words) took 0.3s, 1143886 effective words/s\n",
      "2020-07-27 15:52:24,410 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:52:24,415 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:52:24,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:52:24,418 : INFO : EPOCH - 3 : training on 365311 raw words (305571 effective words) took 0.3s, 1085559 effective words/s\n",
      "2020-07-27 15:52:24,678 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:52:24,681 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:52:24,685 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:52:24,685 : INFO : EPOCH - 4 : training on 365311 raw words (305566 effective words) took 0.3s, 1186951 effective words/s\n",
      "2020-07-27 15:52:24,944 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:52:24,951 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:52:24,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:52:24,953 : INFO : EPOCH - 5 : training on 365311 raw words (305559 effective words) took 0.3s, 1197408 effective words/s\n",
      "2020-07-27 15:52:24,954 : INFO : training on a 1826555 raw words (1527381 effective words) took 1.4s, 1087124 effective words/s\n",
      "2020-07-27 15:52:24,955 : INFO : loading projection weights from ./data/sgns.financial.char.bz2\n",
      "2020-07-27 15:55:27,310 : INFO : merged 7470 vectors into (8804, 300) matrix from ./data/sgns.financial.char.bz2\n",
      "2020-07-27 15:55:27,311 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-07-27 15:55:27,311 : INFO : training model with 3 workers on 8804 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-07-27 15:55:27,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:27,571 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:27,573 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:27,574 : INFO : EPOCH - 1 : training on 365311 raw words (305689 effective words) took 0.3s, 1195026 effective words/s\n",
      "2020-07-27 15:55:27,835 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:27,840 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:27,843 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:27,843 : INFO : EPOCH - 2 : training on 365311 raw words (305596 effective words) took 0.3s, 1192279 effective words/s\n",
      "2020-07-27 15:55:28,093 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:28,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:28,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:28,100 : INFO : EPOCH - 3 : training on 365311 raw words (305115 effective words) took 0.2s, 1238253 effective words/s\n",
      "2020-07-27 15:55:28,359 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:28,365 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:28,367 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:28,368 : INFO : EPOCH - 4 : training on 365311 raw words (305558 effective words) took 0.3s, 1185798 effective words/s\n",
      "2020-07-27 15:55:28,640 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:28,643 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:28,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:28,645 : INFO : EPOCH - 5 : training on 365311 raw words (305502 effective words) took 0.3s, 1142350 effective words/s\n",
      "2020-07-27 15:55:28,942 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:28,948 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:28,949 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:28,949 : INFO : EPOCH - 6 : training on 365311 raw words (305510 effective words) took 0.3s, 1038060 effective words/s\n",
      "2020-07-27 15:55:29,205 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:29,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:29,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:29,211 : INFO : EPOCH - 7 : training on 365311 raw words (305445 effective words) took 0.3s, 1216657 effective words/s\n",
      "2020-07-27 15:55:29,464 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:29,469 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:29,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:29,474 : INFO : EPOCH - 8 : training on 365311 raw words (305489 effective words) took 0.3s, 1192742 effective words/s\n",
      "2020-07-27 15:55:29,739 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:29,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-27 15:55:29,747 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:29,747 : INFO : EPOCH - 9 : training on 365311 raw words (305391 effective words) took 0.3s, 1163226 effective words/s\n",
      "2020-07-27 15:55:30,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-27 15:55:30,026 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 15:55:30,028 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-27 15:55:30,028 : INFO : EPOCH - 10 : training on 365311 raw words (305573 effective words) took 0.3s, 1125331 effective words/s\n",
      "2020-07-27 15:55:30,029 : INFO : training on a 3653110 raw words (3054868 effective words) took 2.7s, 1124149 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3054868, 3653110)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v = gensim.models.Word2Vec(texts_title_east, size=300)\n",
    "model_w2v.intersect_word2vec_format('./data/sgns.financial.char.bz2',\n",
    "                                lockf=1.0,\n",
    "                                encoding='utf8',\n",
    "                                )\n",
    "model_w2v.train(texts_title_east,total_examples=model_w2v.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pass_through = lambda x:x\n",
    "tfidf = TfidfVectorizer(analyzer=pass_through)\n",
    "title_vectorizer = tfidf.fit_transform(texts_title_east)\n",
    "\n",
    "#to dense\n",
    "title_east_tfidf = title_vectorizer.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get weight matrix\n",
    "title_east_weight = []\n",
    "for i in range(len(title_east_tfidf)):\n",
    "    idf_list = title_east_tfidf[i]\n",
    "    temp = []\n",
    "    for word in texts_title_east[i]:\n",
    "        temp.append(idf_list[tfidf.vocabulary_[word]])\n",
    "    temp = np.array(temp)\n",
    "    title_east_weight.append(temp)\n",
    "title_east_weight = np.array(title_east_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/57222 [00:00<?, ?it/s]<ipython-input-9-10763caeac09>:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  sentence_w2v.append(model_w2v[word])\n",
      "100%|██████████| 57222/57222 [00:01<00:00, 29361.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#get word2vec matrix\n",
    "title_east_w2v = []\n",
    "for i in tqdm(range(len(texts_title_east))):\n",
    "    sentence_w2v = []\n",
    "    for word in texts_title_east[i]:\n",
    "        if word not in model_w2v.wv.vocab:\n",
    "            sentence_w2v.append(np.zeros((300,)))\n",
    "        else:\n",
    "            sentence_w2v.append(model_w2v[word])\n",
    "    sentence_w2v = np.array(sentence_w2v)\n",
    "    title_east_w2v.append(sentence_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute sentence embedding\n",
    "def compute_weighted_centroids(X, weights):\n",
    "    centroids = []\n",
    "    for question_vector_weight in zip(X, weights):\n",
    "        if question_vector_weight:\n",
    "            question_vectors, question_weights = question_vector_weight\n",
    "            if np.sum(question_weights) != 0:\n",
    "                centroids.append(np.average(question_vectors, weights=question_weights, axis=0))\n",
    "            else:\n",
    "                centroids.append(np.zeros(300,))                    \n",
    "        else:\n",
    "            centroids.append(np.zeros(300,))\n",
    "    return np.array(centroids)\n",
    "\n",
    "title_east_vec = compute_weighted_centroids(title_east_w2v, title_east_weight)\n",
    "title_east_vec = np.stack(title_east_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'title_east_vec' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87625a5b9f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitle_east_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'title_east_vec' is not defined"
     ]
    }
   ],
   "source": [
    "title_east_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('pDL': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595837792272"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}